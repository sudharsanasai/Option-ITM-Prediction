{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import pickle\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading features by importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_by_importance = list(pd.read_csv('feature_counts_10d.csv',header=None)[0])\n",
    "badf = pd.read_csv('data/badf.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_COL = 'target_10d'\n",
    "DATE_COL = 'date'\n",
    "TARGET_THRESHOLD = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def badf_n_top_features(badf,features_by_importance,n,target_col):\n",
    "    columns = ['date']\n",
    "    columns.extend(features_by_importance[:n])\n",
    "    columns.append(target_col)\n",
    "    return badf.loc[:,columns].set_index('date')\n",
    "\n",
    "def convert_target_labels(df,threshold,target_col):\n",
    "    df.loc[df[target_col] > threshold,target_col] = 1\n",
    "    df.loc[df[target_col] <= threshold,target_col] = 0\n",
    "    return df\n",
    "\n",
    "# badf = badf_n_top_features(badf,features_by_importance,50,TARGET_COL)\n",
    "    \n",
    "badf = convert_target_labels(badf,TARGET_THRESHOLD,TARGET_COL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sampling Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>rsi</th>\n",
       "      <th>bb_high</th>\n",
       "      <th>bb_low</th>\n",
       "      <th>atr</th>\n",
       "      <th>...</th>\n",
       "      <th>CDLSPINNINGTOP</th>\n",
       "      <th>CDLSTALLEDPATTERN</th>\n",
       "      <th>CDLSTICKSANDWICH</th>\n",
       "      <th>CDLTAKURI</th>\n",
       "      <th>CDLTASUKIGAP</th>\n",
       "      <th>CDLTHRUSTING</th>\n",
       "      <th>CDLTRISTAR</th>\n",
       "      <th>CDLUNIQUE3RIVER</th>\n",
       "      <th>CDLUPSIDEGAP2CROWS</th>\n",
       "      <th>CDLXSIDEGAP3METHODS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1999-03-10</td>\n",
       "      <td>51.125000</td>\n",
       "      <td>51.156250</td>\n",
       "      <td>50.281250</td>\n",
       "      <td>44.523476</td>\n",
       "      <td>5232000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1999-03-11</td>\n",
       "      <td>51.437500</td>\n",
       "      <td>51.734375</td>\n",
       "      <td>50.312500</td>\n",
       "      <td>44.741455</td>\n",
       "      <td>9688600</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1999-03-12</td>\n",
       "      <td>51.125000</td>\n",
       "      <td>51.156250</td>\n",
       "      <td>49.656250</td>\n",
       "      <td>43.651520</td>\n",
       "      <td>8743600</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1999-03-15</td>\n",
       "      <td>50.437500</td>\n",
       "      <td>51.562500</td>\n",
       "      <td>49.906250</td>\n",
       "      <td>44.904942</td>\n",
       "      <td>6369000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1999-03-16</td>\n",
       "      <td>51.718750</td>\n",
       "      <td>52.156250</td>\n",
       "      <td>51.156250</td>\n",
       "      <td>45.286419</td>\n",
       "      <td>4905800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5453</th>\n",
       "      <td>2020-11-06</td>\n",
       "      <td>293.709991</td>\n",
       "      <td>295.359985</td>\n",
       "      <td>289.829987</td>\n",
       "      <td>294.609985</td>\n",
       "      <td>40959800</td>\n",
       "      <td>61.558118</td>\n",
       "      <td>0.018474</td>\n",
       "      <td>0.086460</td>\n",
       "      <td>7.118914</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5454</th>\n",
       "      <td>2020-11-09</td>\n",
       "      <td>297.649994</td>\n",
       "      <td>299.140015</td>\n",
       "      <td>288.119995</td>\n",
       "      <td>288.589996</td>\n",
       "      <td>86537100</td>\n",
       "      <td>55.533787</td>\n",
       "      <td>0.035281</td>\n",
       "      <td>0.068016</td>\n",
       "      <td>7.397564</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5455</th>\n",
       "      <td>2020-11-10</td>\n",
       "      <td>285.170013</td>\n",
       "      <td>286.660004</td>\n",
       "      <td>280.619995</td>\n",
       "      <td>283.420013</td>\n",
       "      <td>69024900</td>\n",
       "      <td>50.924553</td>\n",
       "      <td>0.047727</td>\n",
       "      <td>0.051233</td>\n",
       "      <td>7.438453</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5456</th>\n",
       "      <td>2020-11-11</td>\n",
       "      <td>286.029999</td>\n",
       "      <td>290.250000</td>\n",
       "      <td>283.380005</td>\n",
       "      <td>289.760010</td>\n",
       "      <td>36102900</td>\n",
       "      <td>55.772406</td>\n",
       "      <td>0.026095</td>\n",
       "      <td>0.070325</td>\n",
       "      <td>7.397849</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5457</th>\n",
       "      <td>2020-11-12</td>\n",
       "      <td>290.760010</td>\n",
       "      <td>292.070007</td>\n",
       "      <td>287.250000</td>\n",
       "      <td>288.399994</td>\n",
       "      <td>34414600</td>\n",
       "      <td>54.528049</td>\n",
       "      <td>0.029810</td>\n",
       "      <td>0.066046</td>\n",
       "      <td>7.213717</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5458 rows Ã— 170 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            date        open        high         low       close    volume  \\\n",
       "0     1999-03-10   51.125000   51.156250   50.281250   44.523476   5232000   \n",
       "1     1999-03-11   51.437500   51.734375   50.312500   44.741455   9688600   \n",
       "2     1999-03-12   51.125000   51.156250   49.656250   43.651520   8743600   \n",
       "3     1999-03-15   50.437500   51.562500   49.906250   44.904942   6369000   \n",
       "4     1999-03-16   51.718750   52.156250   51.156250   45.286419   4905800   \n",
       "...          ...         ...         ...         ...         ...       ...   \n",
       "5453  2020-11-06  293.709991  295.359985  289.829987  294.609985  40959800   \n",
       "5454  2020-11-09  297.649994  299.140015  288.119995  288.589996  86537100   \n",
       "5455  2020-11-10  285.170013  286.660004  280.619995  283.420013  69024900   \n",
       "5456  2020-11-11  286.029999  290.250000  283.380005  289.760010  36102900   \n",
       "5457  2020-11-12  290.760010  292.070007  287.250000  288.399994  34414600   \n",
       "\n",
       "            rsi   bb_high    bb_low       atr  ...  CDLSPINNINGTOP  \\\n",
       "0           NaN       NaN       NaN       NaN  ...               0   \n",
       "1           NaN       NaN       NaN       NaN  ...               0   \n",
       "2           NaN       NaN       NaN       NaN  ...               0   \n",
       "3           NaN       NaN       NaN       NaN  ...               0   \n",
       "4           NaN       NaN       NaN       NaN  ...               0   \n",
       "...         ...       ...       ...       ...  ...             ...   \n",
       "5453  61.558118  0.018474  0.086460  7.118914  ...               0   \n",
       "5454  55.533787  0.035281  0.068016  7.397564  ...               0   \n",
       "5455  50.924553  0.047727  0.051233  7.438453  ...               0   \n",
       "5456  55.772406  0.026095  0.070325  7.397849  ...               0   \n",
       "5457  54.528049  0.029810  0.066046  7.213717  ...               0   \n",
       "\n",
       "      CDLSTALLEDPATTERN  CDLSTICKSANDWICH  CDLTAKURI  CDLTASUKIGAP  \\\n",
       "0                     0                 0          0             0   \n",
       "1                     0                 0          0             0   \n",
       "2                     0                 0          0             0   \n",
       "3                     0                 0          0             0   \n",
       "4                     0                 0          0             0   \n",
       "...                 ...               ...        ...           ...   \n",
       "5453                  0                 0          0             0   \n",
       "5454                  0                 0          0             0   \n",
       "5455                  0                 0          0             0   \n",
       "5456                  0                 0          0             0   \n",
       "5457                  0                 0          0             0   \n",
       "\n",
       "      CDLTHRUSTING  CDLTRISTAR  CDLUNIQUE3RIVER  CDLUPSIDEGAP2CROWS  \\\n",
       "0                0           0                0                   0   \n",
       "1                0           0                0                   0   \n",
       "2                0           0                0                   0   \n",
       "3                0           0                0                   0   \n",
       "4                0           0                0                   0   \n",
       "...            ...         ...              ...                 ...   \n",
       "5453             0           0                0                   0   \n",
       "5454             0           0                0                   0   \n",
       "5455             0           0                0                   0   \n",
       "5456             0           0                0                   0   \n",
       "5457             0           0                0                   0   \n",
       "\n",
       "      CDLXSIDEGAP3METHODS  \n",
       "0                       0  \n",
       "1                       0  \n",
       "2                       0  \n",
       "3                       0  \n",
       "4                       0  \n",
       "...                   ...  \n",
       "5453                    0  \n",
       "5454                    0  \n",
       "5455                    0  \n",
       "5456                    0  \n",
       "5457                    0  \n",
       "\n",
       "[5458 rows x 170 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "badf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Priciples**\n",
    "\n",
    "1) Test set should be of the date after the training set. \n",
    "\n",
    "2) Missing Value Strategy () --? min?\n",
    "\n",
    "3) Training Set Window - 1000 days\n",
    "\n",
    "4) Test set - 300 days\n",
    "\n",
    "5) Remove 2020 data\n",
    "\n",
    "6) Holdout set 2019 data\n",
    "\n",
    "**Variations** \n",
    "\n",
    "1) Normalized\n",
    "\n",
    "2) Polynomial and Logarithmic Features\n",
    "\n",
    "2) Non Normalized\n",
    "\n",
    "**Model**\n",
    "\n",
    "1) Logistic Regression\n",
    "\n",
    "2) Decision Tree\n",
    "\n",
    "3) Random Forest\n",
    "\n",
    "4) SVM\n",
    "\n",
    "5) Gradient Boosting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_datasets(df,no_of_features,ignore_after,holdout_set_after,test_set_rows,train_set_rows,target,features_by_importance,cross_val_days):\n",
    "    df = badf_n_top_features(df,features_by_importance,no_of_features,target)\n",
    "    df = df[df.index <ignore_after]\n",
    "    df['target_10d'] = df['target_10d'].astype('int')\n",
    "\n",
    "    df_holdout = df[df.index >= holdout_set_after]\n",
    "    df = df[df.index < holdout_set_after]\n",
    "\n",
    "    for column in df.columns:\n",
    "        if df[column].isna().any():\n",
    "            df[column].fillna(df[column].min(),inplace=True)\n",
    "        \n",
    "    train_set = df[-test_set_rows-train_set_rows-cross_val_days:-test_set_rows-cross_val_days-1]\n",
    "    test_set = df[-test_set_rows-cross_val_days-1:-cross_val_days-1]\n",
    "    train_set_X = train_set.drop(columns='target_10d')\n",
    "    train_set_y = train_set['target_10d']\n",
    "    test_set_X = test_set.drop(columns='target_10d')\n",
    "    test_set_y = test_set['target_10d']\n",
    "    \n",
    "    return df,train_set_X,test_set_X,train_set_y,test_set_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df,train_set_X,test_set_X,train_set_y,test_set_y = prep_datasets(badf,50,'2020-01-01','2019-01-01',150,1000,'target_10d',features_by_importance,150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ver = '1'\n",
    "model_dict = {}\n",
    "model_dict['models'] = []\n",
    "top_n = [5,10,15,20,25,30,35,40,45,50]\n",
    "cross_val = range(0,1500,150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = [100, 10, 1.0, 0.1, 0.01]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No of Features: 5, Time Taken: 0:00:01.031361\n",
      "No of Features: 10, Time Taken: 0:00:01.162737\n",
      "No of Features: 15, Time Taken: 0:00:01.324077\n",
      "No of Features: 20, Time Taken: 0:00:01.719213\n",
      "No of Features: 25, Time Taken: 0:00:02.510121\n",
      "No of Features: 30, Time Taken: 0:00:03.445817\n",
      "No of Features: 35, Time Taken: 0:00:03.801287\n",
      "No of Features: 40, Time Taken: 0:00:06.281485\n",
      "No of Features: 45, Time Taken: 0:00:06.519222\n",
      "No of Features: 50, Time Taken: 0:00:06.586983\n"
     ]
    }
   ],
   "source": [
    "for no_of_features in top_n:\n",
    "    a = datetime.datetime.now()\n",
    "    for c in C:\n",
    "        model_params = {}\n",
    "        model_params['Feature Transform'] = None\n",
    "        model_params['model_type'] = 'Logistic Regression'\n",
    "        model_params['C'] = c\n",
    "        model_params['no_of_features'] = no_of_features\n",
    "        model_params['CrossValDays'] = []\n",
    "        model_params['Confusion Matrices'] = []\n",
    "        model_params['AUC Scores'] = []\n",
    "        model_params['TN'], model_params['FP'],model_params['FN'], model_params['TP'] = [],[],[],[]\n",
    "        for cross_val_days in cross_val:\n",
    "            df,train_set_X,test_set_X,train_set_y,test_set_y = prep_datasets(badf,no_of_features,'2020-01-01','2019-01-01',300,1000,'target_10d',features_by_importance,cross_val_days)\n",
    "            model = LogisticRegression(C=c,max_iter=1000)\n",
    "            model.fit(train_set_X,train_set_y)\n",
    "            y_pred = model.predict(test_set_X)\n",
    "            y_pred_proba = model.predict_proba(test_set_X)\n",
    "            \n",
    "            model_params['CrossValDays'].append(cross_val_days)\n",
    "            \n",
    "            model_params['Confusion Matrices'].append(confusion_matrix(test_set_y,y_pred))\n",
    "            model_params['TN'].append(confusion_matrix(test_set_y,y_pred).ravel()[0])\n",
    "            model_params['FP'].append(confusion_matrix(test_set_y,y_pred).ravel()[1])\n",
    "            model_params['FN'].append(confusion_matrix(test_set_y,y_pred).ravel()[2])\n",
    "            model_params['TP'].append(confusion_matrix(test_set_y,y_pred).ravel()[3])\n",
    "            model_params['AUC Scores'].append(roc_auc_score(test_set_y,y_pred_proba[:,1]))\n",
    "            \n",
    "        model_params['AUC Score'] = sum(model_params['AUC Scores'])/len(model_params['AUC Scores'])\n",
    "        model_dict['models'].append(model_params)\n",
    "    b = datetime.datetime.now()\n",
    "    print('No of Features: {}, Time Taken: {}'.format(no_of_features,b-a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(model_dict,open('model_perf.pkl_'+ver,'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_df = pd.DataFrame(model_dict['models'])\n",
    "model_df.to_csv('model_perf.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No of Features: 5, Time Taken: 0:00:01.101910\n",
      "No of Features: 10, Time Taken: 0:00:01.385664\n",
      "No of Features: 15, Time Taken: 0:00:01.556189\n",
      "No of Features: 20, Time Taken: 0:00:01.773286\n",
      "No of Features: 25, Time Taken: 0:00:01.945496\n",
      "No of Features: 30, Time Taken: 0:00:02.100055\n",
      "No of Features: 35, Time Taken: 0:00:02.265169\n",
      "No of Features: 40, Time Taken: 0:00:02.306235\n",
      "No of Features: 45, Time Taken: 0:00:02.393168\n",
      "No of Features: 50, Time Taken: 0:00:02.436649\n"
     ]
    }
   ],
   "source": [
    "min_leaves_comb = [2,4,8,16,32,64,128,256]\n",
    "\n",
    "for no_of_features in top_n:\n",
    "    a = datetime.datetime.now()\n",
    "    for min_leaves in min_leaves_comb:\n",
    "        model_params = {}\n",
    "        model_params['Feature Transform'] = None\n",
    "        model_params['model_type'] = 'Decision Tree Regressor'\n",
    "        model_params['Min Leaves'] = min_leaves\n",
    "        model_params['no_of_features'] = no_of_features\n",
    "        model_params['CrossValDays'] = []\n",
    "        model_params['Confusion Matrices'] = []\n",
    "        model_params['AUC Scores'] = []\n",
    "        model_params['TN'], model_params['FP'],model_params['FN'], model_params['TP'] = [],[],[],[]\n",
    "        for cross_val_days in cross_val:\n",
    "            df,train_set_X,test_set_X,train_set_y,test_set_y = prep_datasets(badf,no_of_features,'2020-01-01','2019-01-01',300,1000,'target_10d',features_by_importance,cross_val_days)\n",
    "            model = DecisionTreeClassifier(min_samples_leaf=min_leaves)\n",
    "            model.fit(train_set_X,train_set_y)\n",
    "            y_pred = model.predict(test_set_X)\n",
    "            y_pred_proba = model.predict_proba(test_set_X)\n",
    "            model_params['CrossValDays'].append(cross_val_days)\n",
    "            \n",
    "            model_params['Confusion Matrices'].append(confusion_matrix(test_set_y,y_pred))\n",
    "            model_params['TN'].append(confusion_matrix(test_set_y,y_pred).ravel()[0])\n",
    "            model_params['FP'].append(confusion_matrix(test_set_y,y_pred).ravel()[1])\n",
    "            model_params['FN'].append(confusion_matrix(test_set_y,y_pred).ravel()[2])\n",
    "            model_params['TP'].append(confusion_matrix(test_set_y,y_pred).ravel()[3])\n",
    "            model_params['AUC Scores'].append(roc_auc_score(test_set_y,y_pred_proba[:,-1]))\n",
    "\n",
    "        model_params['AUC Score'] = sum(model_params['AUC Scores'])/len(model_params['AUC Scores'])\n",
    "        model_dict['models'].append(model_params)\n",
    "    b = datetime.datetime.now()\n",
    "    print('No of Features: {}, Time Taken: {}'.format(no_of_features,b-a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_df = pd.DataFrame(model_dict['models'])\n",
    "model_df.to_csv('model_perf.csv')\n",
    "pickle.dump(model_dict,open('model_perf.pkl_'+ver,'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No of Features: 5, Time Taken: 0:00:14.098998\n",
      "No of Features: 10, Time Taken: 0:00:14.809068\n",
      "No of Features: 15, Time Taken: 0:00:15.502108\n",
      "No of Features: 20, Time Taken: 0:00:19.399188\n",
      "No of Features: 25, Time Taken: 0:00:19.425201\n",
      "No of Features: 30, Time Taken: 0:00:20.176708\n",
      "No of Features: 35, Time Taken: 0:00:21.367370\n",
      "No of Features: 40, Time Taken: 0:00:21.705380\n",
      "No of Features: 45, Time Taken: 0:00:23.235123\n",
      "No of Features: 50, Time Taken: 0:00:23.814029\n"
     ]
    }
   ],
   "source": [
    "n_neighbors = range(1, 21, 2)\n",
    "weights = ['uniform', 'distance']\n",
    "metrics = ['euclidean', 'manhattan', 'minkowski']\n",
    "\n",
    "for no_of_features in top_n:\n",
    "    a = datetime.datetime.now()\n",
    "    for n_neighbor in n_neighbors:\n",
    "        for weight in weights:\n",
    "            for metric in metrics:\n",
    "                model_params = {}\n",
    "                model_params['Feature Transform'] = None\n",
    "                model_params['model_type'] = 'K Neighbors'\n",
    "                model_params['Number of Neighbors'] = n_neighbor\n",
    "                model_params['metric'] = metric\n",
    "                model_params['weight'] = weight\n",
    "                model_params['no_of_features'] = no_of_features\n",
    "                model_params['CrossValDays'] = []\n",
    "                model_params['Confusion Matrices'] = []\n",
    "                model_params['AUC Scores'] = []\n",
    "                model_params['TN'], model_params['FP'],model_params['FN'], model_params['TP'] = [],[],[],[]\n",
    "                for cross_val_days in cross_val:\n",
    "                    df,train_set_X,test_set_X,train_set_y,test_set_y = prep_datasets(badf,no_of_features,'2020-01-01','2019-01-01',300,1000,'target_10d',features_by_importance,cross_val_days)\n",
    "                    model = KNeighborsClassifier(n_neighbors=n_neighbor,weights=weight,metric=metric,n_jobs=-1)\n",
    "                    model.fit(train_set_X,train_set_y)\n",
    "                    y_pred = model.predict(test_set_X)\n",
    "                    y_pred_proba = model.predict_proba(test_set_X)\n",
    "                    model_params['CrossValDays'].append(cross_val_days)\n",
    "            \n",
    "                    model_params['Confusion Matrices'].append(confusion_matrix(test_set_y,y_pred))\n",
    "                    model_params['TN'].append(confusion_matrix(test_set_y,y_pred).ravel()[0])\n",
    "                    model_params['FP'].append(confusion_matrix(test_set_y,y_pred).ravel()[1])\n",
    "                    model_params['FN'].append(confusion_matrix(test_set_y,y_pred).ravel()[2])\n",
    "                    model_params['TP'].append(confusion_matrix(test_set_y,y_pred).ravel()[3])\n",
    "                    model_params['AUC Scores'].append(roc_auc_score(test_set_y,y_pred_proba[:,-1]))\n",
    "\n",
    "                model_params['AUC Score'] = sum(model_params['AUC Scores'])/len(model_params['AUC Scores'])\n",
    "                model_dict['models'].append(model_params)\n",
    "    b = datetime.datetime.now()\n",
    "    print('No of Features: {}, Time Taken: {}'.format(no_of_features,b-a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_df = pd.DataFrame(model_dict['models'])\n",
    "model_df.to_csv('model_perf.csv')\n",
    "pickle.dump(model_dict,open('model_perf.pkl_'+ver,'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 500 and 1000 trees were removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No of Features: 5, No of Trees: 100, Time Taken: 0:00:17.162661\n",
      "No of Features: 5, No of Trees: 200, Time Taken: 0:00:31.098539\n",
      "No of Features: 5, No of Trees: 300, Time Taken: 0:00:47.014383\n",
      "No of Features: 10, No of Trees: 100, Time Taken: 0:00:16.883892\n",
      "No of Features: 10, No of Trees: 200, Time Taken: 0:00:32.182602\n",
      "No of Features: 10, No of Trees: 300, Time Taken: 0:00:45.821934\n",
      "No of Features: 15, No of Trees: 100, Time Taken: 0:00:16.680396\n",
      "No of Features: 15, No of Trees: 200, Time Taken: 0:00:31.835552\n",
      "No of Features: 15, No of Trees: 300, Time Taken: 0:00:46.321055\n",
      "No of Features: 20, No of Trees: 100, Time Taken: 0:00:17.030165\n",
      "No of Features: 20, No of Trees: 200, Time Taken: 0:00:31.758068\n",
      "No of Features: 20, No of Trees: 300, Time Taken: 0:00:46.493627\n",
      "No of Features: 25, No of Trees: 100, Time Taken: 0:00:17.670642\n",
      "No of Features: 25, No of Trees: 200, Time Taken: 0:00:32.624058\n",
      "No of Features: 25, No of Trees: 300, Time Taken: 0:00:45.795809\n",
      "No of Features: 30, No of Trees: 100, Time Taken: 0:00:17.134856\n",
      "No of Features: 30, No of Trees: 200, Time Taken: 0:00:31.053758\n",
      "No of Features: 30, No of Trees: 300, Time Taken: 0:00:45.280762\n",
      "No of Features: 35, No of Trees: 100, Time Taken: 0:00:17.255941\n",
      "No of Features: 35, No of Trees: 200, Time Taken: 0:00:31.768995\n",
      "No of Features: 35, No of Trees: 300, Time Taken: 0:00:47.041673\n",
      "No of Features: 40, No of Trees: 100, Time Taken: 0:00:17.272902\n",
      "No of Features: 40, No of Trees: 200, Time Taken: 0:00:32.008146\n",
      "No of Features: 40, No of Trees: 300, Time Taken: 0:00:47.872555\n",
      "No of Features: 45, No of Trees: 100, Time Taken: 0:00:18.358382\n",
      "No of Features: 45, No of Trees: 200, Time Taken: 0:00:31.699298\n",
      "No of Features: 45, No of Trees: 300, Time Taken: 0:00:46.863806\n",
      "No of Features: 50, No of Trees: 100, Time Taken: 0:00:18.035162\n",
      "No of Features: 50, No of Trees: 200, Time Taken: 0:00:32.412758\n",
      "No of Features: 50, No of Trees: 300, Time Taken: 0:00:46.803463\n"
     ]
    }
   ],
   "source": [
    "min_leaves_comb = range(1,20,2)\n",
    "no_of_trees_comb = [100,200,300]\n",
    "\n",
    "for no_of_features in top_n:\n",
    "    a = datetime.datetime.now()\n",
    "    for no_of_trees in no_of_trees_comb:\n",
    "        a = datetime.datetime.now()\n",
    "        for min_leaves in min_leaves_comb:\n",
    "            model_params = {}\n",
    "            model_params['Feature Transform'] = None\n",
    "            model_params['model_type'] = 'Random Forest'\n",
    "            model_params['no_of_trees'] = no_of_trees\n",
    "            model_params['min_leaves'] = min_leaves\n",
    "            model_params['no_of_features'] = no_of_features\n",
    "            model_params['CrossValDays'] = []\n",
    "            model_params['Confusion Matrices'] = []\n",
    "            model_params['AUC Scores'] = []\n",
    "            model_params['TN'], model_params['FP'],model_params['FN'], model_params['TP'] = [],[],[],[]\n",
    "\n",
    "            for cross_val_days in cross_val:\n",
    "                df,train_set_X,test_set_X,train_set_y,test_set_y = prep_datasets(badf,no_of_features,'2020-01-01','2019-01-01',300,1000,'target_10d',features_by_importance,cross_val_days)\n",
    "                model = RandomForestClassifier(n_estimators=no_of_trees,min_samples_leaf=min_leaves,n_jobs=-1)\n",
    "                model.fit(train_set_X,train_set_y)\n",
    "                y_pred = model.predict(test_set_X)\n",
    "                y_pred_proba = model.predict_proba(test_set_X)\n",
    "                \n",
    "                model_params['CrossValDays'].append(cross_val_days)\n",
    "            \n",
    "                model_params['Confusion Matrices'].append(confusion_matrix(test_set_y,y_pred))\n",
    "                model_params['TN'].append(confusion_matrix(test_set_y,y_pred).ravel()[0])\n",
    "                model_params['FP'].append(confusion_matrix(test_set_y,y_pred).ravel()[1])\n",
    "                model_params['FN'].append(confusion_matrix(test_set_y,y_pred).ravel()[2])\n",
    "                model_params['TP'].append(confusion_matrix(test_set_y,y_pred).ravel()[3])\n",
    "                model_params['AUC Scores'].append(roc_auc_score(test_set_y,y_pred_proba[:,-1]))\n",
    "                \n",
    "            model_params['AUC Score'] = sum(model_params['AUC Scores'])/len(model_params['AUC Scores'])\n",
    "            model_dict['models'].append(model_params)\n",
    "        b = datetime.datetime.now()\n",
    "        print('No of Features: {}, No of Trees: {}, Time Taken: {}'.format(no_of_features,no_of_trees,b-a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_df = pd.DataFrame(model_dict['models'])\n",
    "model_df.to_csv('model_perf.csv')\n",
    "pickle.dump(model_dict,open('model_perf.pkl_'+ver,'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "C_comb = [0.01,0.1,1,10]\n",
    "kernel_comb = ['linear', 'poly', 'rbf', 'sigmoid']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#C = 100 was removed because of time constraint more than 3 minute per run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No of Features: 5, C : 0.01, Time Taken: 0:00:03.573751\n",
      "No of Features: 5, C : 0.1, Time Taken: 0:00:03.560154\n",
      "No of Features: 5, C : 1, Time Taken: 0:00:03.577399\n",
      "No of Features: 5, C : 10, Time Taken: 0:00:03.846403\n",
      "No of Features: 10, C : 0.01, Time Taken: 0:00:04.325778\n",
      "No of Features: 10, C : 0.1, Time Taken: 0:00:04.330179\n",
      "No of Features: 10, C : 1, Time Taken: 0:00:04.511310\n",
      "No of Features: 10, C : 10, Time Taken: 0:00:06.037262\n",
      "No of Features: 15, C : 0.01, Time Taken: 0:00:05.004194\n",
      "No of Features: 15, C : 0.1, Time Taken: 0:00:05.117459\n",
      "No of Features: 15, C : 1, Time Taken: 0:00:05.234671\n",
      "No of Features: 15, C : 10, Time Taken: 0:00:06.616879\n",
      "No of Features: 20, C : 0.01, Time Taken: 0:00:06.320526\n",
      "No of Features: 20, C : 0.1, Time Taken: 0:00:09.430450\n",
      "No of Features: 20, C : 1, Time Taken: 0:00:35.217844\n",
      "No of Features: 20, C : 10, Time Taken: 0:04:10.701088\n",
      "No of Features: 25, C : 0.01, Time Taken: 0:00:07.855409\n",
      "No of Features: 25, C : 0.1, Time Taken: 0:00:11.804938\n",
      "No of Features: 25, C : 1, Time Taken: 0:00:50.431377\n",
      "No of Features: 25, C : 10, Time Taken: 0:06:40.319244\n",
      "No of Features: 30, C : 0.01, Time Taken: 0:00:09.279151\n",
      "No of Features: 30, C : 0.1, Time Taken: 0:00:16.889424\n",
      "No of Features: 30, C : 1, Time Taken: 0:01:13.062247\n",
      "No of Features: 30, C : 10, Time Taken: 0:09:09.441956\n",
      "No of Features: 35, C : 0.01, Time Taken: 0:00:10.919786\n",
      "No of Features: 35, C : 0.1, Time Taken: 0:00:17.678551\n",
      "No of Features: 35, C : 1, Time Taken: 0:01:12.544855\n",
      "No of Features: 35, C : 10, Time Taken: 0:10:22.796957\n",
      "No of Features: 40, C : 0.01, Time Taken: 0:00:12.438405\n",
      "No of Features: 40, C : 0.1, Time Taken: 0:00:18.804262\n",
      "No of Features: 40, C : 1, Time Taken: 0:01:16.759668\n",
      "No of Features: 40, C : 10, Time Taken: 0:10:38.865238\n",
      "No of Features: 45, C : 0.01, Time Taken: 0:00:12.453898\n",
      "No of Features: 45, C : 0.1, Time Taken: 0:00:19.110353\n",
      "No of Features: 45, C : 1, Time Taken: 0:01:11.055371\n",
      "No of Features: 45, C : 10, Time Taken: 0:11:06.353808\n",
      "No of Features: 50, C : 0.01, Time Taken: 0:00:14.236005\n",
      "No of Features: 50, C : 0.1, Time Taken: 0:00:20.731270\n",
      "No of Features: 50, C : 1, Time Taken: 0:01:16.453926\n",
      "No of Features: 50, C : 10, Time Taken: 0:10:41.610374\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for no_of_features in top_n:\n",
    "    \n",
    "    for C in C_comb:\n",
    "        a = datetime.datetime.now()\n",
    "        for kernel in kernel_comb:\n",
    "            model_params = {}\n",
    "            model_params['Feature Transform'] = None\n",
    "            model_params['model_type'] = 'Support Vector Classifier'\n",
    "            model_params['C'] = C\n",
    "            model_params['Kernel'] = kernel\n",
    "            model_params['no_of_features'] = no_of_features\n",
    "            model_params['CrossValDays'] = []\n",
    "            model_params['Confusion Matrices'] = []\n",
    "            model_params['AUC Scores'] = []\n",
    "            model_params['TN'], model_params['FP'],model_params['FN'], model_params['TP'] = [],[],[],[]\n",
    "\n",
    "            for cross_val_days in cross_val:\n",
    "                df,train_set_X,test_set_X,train_set_y,test_set_y = prep_datasets(badf,no_of_features,'2020-01-01','2019-01-01',300,1000,'target_10d',features_by_importance,cross_val_days)\n",
    "                model = SVC(C=C,kernel=kernel,probability=True)\n",
    "                model.fit(train_set_X,train_set_y)\n",
    "                y_pred = model.predict(test_set_X)\n",
    "                y_pred_proba = model.predict_proba(test_set_X)\n",
    "                model_params['CrossValDays'].append(cross_val_days)\n",
    "                model_params['Confusion Matrices'].append(confusion_matrix(test_set_y,y_pred))\n",
    "                model_params['TN'].append(confusion_matrix(test_set_y,y_pred).ravel()[0])\n",
    "                model_params['FP'].append(confusion_matrix(test_set_y,y_pred).ravel()[1])\n",
    "                model_params['FN'].append(confusion_matrix(test_set_y,y_pred).ravel()[2])\n",
    "                model_params['TP'].append(confusion_matrix(test_set_y,y_pred).ravel()[3])\n",
    "                model_params['AUC Scores'].append(roc_auc_score(test_set_y,y_pred_proba[:,-1]))\n",
    "\n",
    "            model_params['AUC Score'] = sum(model_params['AUC Scores'])/len(model_params['AUC Scores'])\n",
    "            model_dict['models'].append(model_params)\n",
    "\n",
    "        b = datetime.datetime.now()\n",
    "        print('No of Features: {}, C : {}, Time Taken: {}'.format(no_of_features,C,b-a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_df = pd.DataFrame(model_dict['models'])\n",
    "model_df.to_csv('model_perf.csv')\n",
    "pickle.dump(model_dict,open('model_perf.pkl_'+ver,'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
